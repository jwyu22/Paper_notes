---
title: "High-throughput Genomic Paper Notes"
author: "Jonathan Yu"
date: "November 19, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Smyth, 2004

- Smyth, Gordon K. "Linear Models and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments." Statistical Applications in Genetics and Molecular Biology 3 (2004)


Smyth proposed an extended hierarchical model for microarray experiments. Given the large number of gene-wise linear model fits, a moderated t, using the classical t-statistic as a basis, uses a posterior variance instead of the sample variance to detect differences in gene expressions. The innovation is that prior information is assumed for the variance among the genes and thus, the posterior can be derived to shrink the observed variances. The hypothesis would be that model coefficient that distinguishes differences between group sis zero. It was noted that when gene identifcation using either the current B-statistic model or the moderated t-statistic, they would lead to similar results albeit the t-statistic method would require less knowledge of the parameters. Simulation shows that a moderated t has a lower false discovery rate than other methods and that it can be used for single and two color microarray experiments. 

### Tusher, Tibshirani, and Chu, 2001

- Tusher, V. G., R. Tibshirani, and G. Chu. "Significance Analysis of Microarrays Applied to the Ionizing Radiation Response." Proceedings of the National Academy of Sciences of the United States of America 98, no. 9 (April 24, 2001)

the method Significance Analysis of Microarrays (SAM) was developed to identify significant changes in the expression of thousand of genes during the many different biological states. A score is assigned to each gene for each change relative to the standard deviation for repeated measurements. The genes with scores above a specified threshold, permutations of the repeated measurements are used to estimate the false discovery rate (FDR). 

First, the score comes from the signal-to-noise ratio where they ccounted for gene-specific fluctionations in the gene expressions d(i) $d(i) = \frac{\bar{x}_I(i) - \bar{x}_U(i)}{s(i) + s_0}$ where the numerator is the difference between the average levels of gene i between two states I and U. The gene variation of the repeated measurement account is used in the denominator. Next, the differences are ranked from highest different to lowest relative differnece. Then, permutations of the repeated measurements were done in order to minimize potential confounding effects from differences between two cell lines. A statistic is calculated for each permutation which will result in a distribution of the relative differences where the mean reltaive difference can be derived. This can then compare and identify induced and repressed genes. Their method "for setting thresholds provides asymmetric cutoffs for induced and repressed genes.". The alternative method, t test, has a symmetric horizontal cutoff. SAM has neither strong or weak family wise error rate. 

### Bolstad et al., 2003

- Bolstad, B. M., R. A. Irizarry, M. Astrand, and T. P. Speed. "A Comparison of Normalization Methods for High Density Oligonucleotide Array Data Based on Variance and Bias." Bioinformatics (Oxford, England) 19, no. 2 (January 22, 2003): 185-93. - Normalization methods description.

Three complete data methods: cyclic loes, contrasted based method, and quantile normalization are different norminalization techniques to reduce obscuring variation between oligonucleotide arrays. Current methods - scaling and non-linear method - utilized by Affymetrix do not reduce the variation as much as the other methods and performed poorly for spike-in regressions. Affymetrix methods do not deal with cases that are non-linear relationships between arrays. The cyclic loess and contrast based method are derived from the M vs A method and could be time consuming. All these methods depend on choice of baselines but the complete data methods performed matter on the matter of variance reduction and bias. 

### Cleveland & Devlin, 1988

- Cleveland, William S, and Susan J Devlin. "Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting." Journal of the American Statistical Association 83, no. 403 (1988): 596-610. - Loess regression. Concept, statistics.

Locally weighted regression (loess) is introduced as a local fitting regression, similar to the least squares, that can be used for data exploration, diagnostic checking  of parametric models and nonparametric regression surface. The dependent variable is smoothed as a function of the independent variable in a moving fashion. They show that this can be used to graph smooth surfaces to help choose a parametric model if needed during the exploratory phase. They then show that the adequacy of the parametric model chosen. Finally, it can be shown that the estimate can be used instead of parametric estimates. The multivariate smoother is an extension of the univariate loess smoother where they use the euclidian distance on each scaled independent variables. Loess is comprised of the weights and the neighborhood size. The M plot can help choose the fraction of points in the neighborhood. Assumption include that the loess estimate be a linear combinaton of the outcome observations, that the outcome is a normal distribution, and the loess estimate is not biased. 

### Altman and Martin, 2015 

- Altman, N. and Krzywinski, M., 2015. Points of Significance: Association, correlation and causation. Nature Methods, 12(10)

Variability affects our internal validity (replication of experiments) and external validity (generalization of experiments to population). A well-designed experiment compromises between internal and external validity - if try to focus too much on one aspect, we lose the ability to have the other. In particular, two prinicples - precision to characterize a sample and variance from different sources together - can be combined into a nested design. Nuisance variation that occur from experimental design should be minimizaed to optimize power whereas those that occur from population should be sampled and quantified to make conclusions and determine uncertainty in the estimates of the conclusions. 

### Tumor Analysis Best Practices Working Group, 2004

- Tumor Analysis Best Practices Working Group. "Expression Profiling--Best Practices for Data Generation and Interpretation in Clinical Trials." Nature Reviews. Genetics 5, no. 3 (March 2004): 229-37. doi:10.1038/nrg1297. 

Microarrays is a widely accepted technological way to analyze mRNA transcript levels for a genome. For this technique, there are a variety of methods methods for data generation and analysis for different experimental platforms: cDNA (sets of plasmids of specific cDNA in gridded liquid aliquots), spotted oligonucleotide (concentration of a known single-stranded sequence obtained from liquid handling on glass slides) and Affymetrix arrays (probes that are synthesized using light-activated chemistry and photolithography to find signals). The Tumor Analysis Best Practices Working Group deteremined the best practices for experimental design, probe-set analysis algorithms, signal/noise assessments and biostatistical methods. For human trials, longitudinal or corss-sectional design was determined best protocol as an experimental design for best power. On the note of technical variabliiltiy, reproducibility as well many other problems should be met with some thresholds mentioned such as 2 standard deviation from mean for scaling factor to normalize chips and percentage of present calls among samples should be within 10%. For signal/noise, they determined that each project will have its own signal/noise optimum and their own method of best analysis and compared the different algorithms (Table 1) on a number of criterions. The note that since 'feature or gene selection is vitally important when microarrays ar eused for differential diagnosis', they recommend users try different statistical methods such as standard parametric tests, non parametric methods, and global/local shrinkage methods. With aggregate gene expressions can help reduce dimension - a prevalent problem for analyzing gene related data - as well as gene selection, multiple testing and collinearity. Finally, they state back-end statistical methods such as data visualization and time-series studies can help with circumvent problems as well. 

### Lipshutz et al., 2005

- Liphsutz, Robert J., Stephen PA Fodor, Thomas R. Gingeras, and David J. Lockhart. "High density synthetic oligonucleotide arrays." Nature genetics 21, no. 1 (1999): 20-24

Lipshutz et al. have developped a tool to collect analyze vast amounts of genetic and cellular information simultaneously from nucleic acid strands. Using photolithography and light-activated chemistry, they are able to take advantage of the complementary properties of genes and the target design of the probes, they were able to design a DNA probe array to obtain complementary sequences and monitor a large amount of expression levels. For example, the array is coated with a chemistry protecting group to prevent DNA deposition and then a mask is placed on top to expose specified regions. A light is used to knocked off exposed protecting group and then add certain solution of nucleitide incubations that hybridize to the nucleitide at that location. This cycle is repeated until a synthesized polynucleotides of about 300,000 are created at specific locations on the 1.28 cm $\times$ 1.28 cm array. The array that can contain approximately 40,000 human genes are used for expression monitoring to understand a gene function. Fluorescence intensity image of the array show perfect match (PM) and mismatch (MM) probe pairs. The difference of PM and MM can help reduce background noise and cross-hybridization in order to detect variants in DNA sequence - to identify the difference and the position of nucleitides. For a single position on the DNA, there can a number of PM/MM probe pairs. 

This technique is useful as there is a need for 'monitoring expression levels of a large number of genes repeatedly, routinely, and reproducibily' that do not need physcial intermediaries such as cDNA, PCR products or clones and the process to prepare, verify and cataloque them. Companies such as Affymetrix are developping software tools to manage, genotype, and sequence analyze these datasets. 

### Watson & Crick, 1953

- Watson, J. D., and F. H. Crick. "Molecular Structure of Nucleic Acids; a Structure for Deoxyribose Nucleic Acid." Nature 171, no. 4356 (April 25, 1953):737-38

After discovering some inconsistensies with current proposed structures of nucleic acid, Watson and Crick proposed a two helical chain structure where each are coiled around the same axis. Using the same chemical assumption of the 3', 5' linkages, the chains run in opposite directions and have bases on the insdie and the phosphates on the outside. In other words, the proposed DNA strucutre is a double-stranded helical model with two sugar-phosphate as backbones on the outside and hydrogen bonds between pairs of nitrogenous bases on the inside. The new feature includes having the two chains held by purine and pyrimidine bases - joined together in pairs by hydrogen-bond. Specifically, regarding bases, specific pairs bond together: adenine (purine) with thymine (pyrimidine), and guanine (purine) with cytosine (pyrimidine). They thank Dr Jerry Donohue and Dr. M. H. F. Wilkins \& Dr. R. E. Franklin for their criticisms and expirments for inspirations. 
